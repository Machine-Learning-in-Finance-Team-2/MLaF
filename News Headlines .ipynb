{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Headlines TF-IDF/Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "import pytrends\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude stop words, punctuation, one letter word \n",
    "stopwords = {'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "                       'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                       'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "                       'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be',\n",
    "                       'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'an',\n",
    "                       'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by',\n",
    "                       'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before',\n",
    "                       'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "                       'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "                       'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                       'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can',\n",
    "                       'just', 'should', 'now', '', 'a', 'Â£m'}\n",
    "\n",
    "# clean up headlines and return set of words in headline\n",
    "def cleanHeadline(hl):\n",
    "    hl = hl.lower()\n",
    "    hl = re.sub(r\"[-,\\\"@\\'?\\.$%_\\d\\+\\:]\", \"\", hl, flags=re.I) # rid of punctuation\n",
    "    hl = re.sub(r\"\\s+\", \" \", hl, flags=re.I) # rid of multiple spaces\n",
    "#     hl = re.split(r\"\\s+\", hl)\n",
    "    return hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for tf-idf\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf-idf scores for each headline\n",
    "def tf_idf(data, col):\n",
    "    #get the text column \n",
    "    # docs = df_sub[df_sub['category'] == \"MONEY\"]['headline'].tolist()\n",
    "    docs = data[col].tolist()\n",
    "\n",
    "    cv=CountVectorizer(max_df=1.0,stop_words=stopwords)\n",
    "    word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "    # compute IDF\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    #generate tf-idf for all headlines in your list. \n",
    "    feature_names=cv.get_feature_names()\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform(docs))\n",
    "\n",
    "    results=[]\n",
    "    for i in range(tf_idf_vector.shape[0]):\n",
    "        # get vector for a single headline\n",
    "        curr_vector=tf_idf_vector[i]\n",
    "\n",
    "        #sort the tf-idf vector by descending order of scores\n",
    "        sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "        #extract only the top n; n here is 20\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,20)\n",
    "\n",
    "        results.append(keywords)\n",
    "\n",
    "    df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keywords and their tf-idf scores into a dict \n",
    "def getAllWords(df):\n",
    "    allWords = {}\n",
    "    for index, row in df.iterrows():\n",
    "        tmpDict = row['keywords']\n",
    "        for word in tmpDict:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = [tmpDict[word]]\n",
    "            else:\n",
    "                allWords[word] += [tmpDict[word]]\n",
    "    return allWords\n",
    "\n",
    "# average tf-idf scores and get top n words\n",
    "def getTopWords(df, n):\n",
    "    word_df = pd.DataFrame.from_dict(getAllWords(df), orient='index')\n",
    "    word_df.transpose()\n",
    "    avg_df = word_df.mean(axis=1) # average tf-idf scores for each word\n",
    "    avg_df = pd.DataFrame(avg_df)\n",
    "    avg_df = avg_df.reset_index()\n",
    "    avg_df.columns = ['word', 'avg_score']\n",
    "    avg_df = avg_df.sort_values('avg_score', ascending=False)\n",
    "    return avg_df['word'].head(n).tolist()\n",
    "    \n",
    "# print(getTopWords(df, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard_df = pd.read_csv('the_guardian_headlines_dataset.csv')\n",
    "guard_df['Article Title'] = guard_df['Article Title'].apply(lambda x:cleanHeadline(x)) # clean headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model\n",
    "def BOW(data, col):\n",
    "    data[col] = data[col].apply(lambda x:cleanHeadline(x)) # clean headlines\n",
    "    docLst = data[col].tolist()\n",
    "    doc = \"\".join(docLst)\n",
    "    count_vec = CountVectorizer(max_df=1.0,stop_words=stopwords)\n",
    "    count_occurs = count_vec.fit_transform([doc])\n",
    "    count_occur_df = pd.DataFrame(\n",
    "        (count, word) for word, count in\n",
    "         zip(count_occurs.toarray().tolist()[0], \n",
    "        count_vec.get_feature_names()))\n",
    "    count_occur_df.columns = ['Word', 'Count']\n",
    "    count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "    return count_occur_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word  Count\n",
      "41967        us   4131\n",
      "33817      says   2743\n",
      "40592     trade   2186\n",
      "3412    billion   1568\n",
      "25351       new   1453\n",
      "...         ...    ...\n",
      "21215  launches    158\n",
      "25999     offer    158\n",
      "5526       case    157\n",
      "19798  investor    156\n",
      "43627      wins    156\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44706"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reuters headlines\n",
    "reuters_bus = pd.read_csv('reutersbusiness_headlines_dataset.csv')\n",
    "r_bus_count = BOW(reuters_bus, 'Article Title')\n",
    "print(r_bus_count.head(200))\n",
    "\n",
    "reuters_bv = pd.read_csv('reutersbreakingviews_headlines_dataset.csv')\n",
    "r_bv_count = BOW(reuters_bv, 'Article Title')\n",
    "print(r_bv_count.head(200))\n",
    "\n",
    "reuters_markets = pd.read_csv('reutersmarkets_headlines_dataset.csv')\n",
    "r_markets_count = BOW(reuters_markets, 'Article Title')\n",
    "print(r_markets_count.head(200))\n",
    "\n",
    "reuters_tech = pd.read_csv('reuterstech_headlines_dataset.csv')\n",
    "r_tech_count = BOW(reuters_tech, 'Article Title')\n",
    "print(r_tech_count.head(200))\n",
    "\n",
    "reuters_wealth = pd.read_csv('reuterswealth_headlines_dataset.csv')\n",
    "r_wealth_count = BOW(reuters_wealth, 'Article Title')\n",
    "print(r_wealth_count.head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word Count\n",
      "41967        us  4131\n",
      "33817      says  2743\n",
      "40592     trade  2186\n",
      "3412    billion  1568\n",
      "25351       new  1453\n",
      "...         ...   ...\n",
      "4586    develop    99\n",
      "4635       didi    99\n",
      "18022     using    98\n",
      "10732    misses    98\n",
      "4062   customer    97\n",
      "\n",
      "[1013 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# get rid of duplicate words and return df of top words\n",
    "def aggWords(datas):\n",
    "    df = pd.DataFrame(columns=['Word', 'Count'])\n",
    "    for data in datas:\n",
    "        df = pd.concat([df, data[0:500]]) # gets top 500 words for each category\n",
    "    df = df.drop_duplicates(subset='Word')\n",
    "    return df\n",
    "\n",
    "# get top 1000 words from reuters headlines \n",
    "df = aggWords([r_bus_count, r_bv_count, r_markets_count, r_tech_count, r_wealth_count])\n",
    "print(df)\n",
    "# convert top words to csv\n",
    "df['Word'].head(1000).to_csv('top1000words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
